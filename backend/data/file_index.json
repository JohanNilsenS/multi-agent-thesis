{
  "app.py": "# app.py\nfrom flask import Flask\nfrom src import create_app\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    app.run(debug=True)",
  "test_embedding.py": "from src.model.utils.embedding import get_embedding_from_llm\n\ndef main():\n    test_text = \"Varf\u00f6r \u00e4r flamingos rosa?\"\n    embedding = get_embedding_from_llm(test_text)\n\n    print(f\"Embedding length: {len(embedding)}\")\n    print(f\"First 5 values: {embedding[:5]}\")\n\nif __name__ == \"__main__\":\n    main()",
  "test_internet.py": "from src.model.tools.internet_search import search_duckduckgo\n\nresults = search_duckduckgo(\"how does langchain memory work?\")\nfor r in results:\n    print(\"\\n---\\n\" + r)\n",
  "test_supervisor.py": "# test_supervisor.py\nfrom src.model.supervisor import SupervisorAgent\nfrom src.model.tools.internet_search import search_duckduckgo\n\ntest_research = True\ntest_git = True\ninternet_search = True\ndef main():\n    supervisor = SupervisorAgent()\n    import os\n    os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n    # After creating the supervisor and running a task\n    git_agent = next(agent for agent in supervisor.agents if agent.name == \"GitAgent\")\n    git_agent.print_file_index_preview()\n\n    if test_research:\n        task = \"research why flamingos are pink\"\n        result = supervisor.delegate(task)\n\n        print(f\"Task Source: {result['source']}\")\n        print(\"Content:\")\n        print(result['content'])\n\n        print(\"\\n--- Running same task again to test DB retrieval ---\\n\")\n        result = supervisor.delegate(task)\n        print(f\"Task Source: {result['source']}\")\n        print(\"Content:\")\n        print(result['content'])\n\n\n    if test_git:\n        print(\"\\n--- Git Commit Summary ---\\n\")\n        result = supervisor.delegate(\"git summary\")\n        print(result)\n\n        print(\"\\n--- Project Overview ---\\n\")\n        result = supervisor.delegate(\"project overview\")\n        print(result)\n\n        print(\"\\n--- Suggestions ---\\n\")\n        result = supervisor.delegate(\"suggest improvement\")\n        print(result)\n\n        print(\"\\n--- Explain Function: process_invoice ---\\n\")\n        git_agent.reindex_files()\n        explanation = git_agent.explain_function(\"explain_function\")\n        print(explanation)\n\n        print(\"\\n--- All Indexed Functions ---\\n\")\n        for func in git_agent.list_all_functions():\n            print(f\"{func['name']}  \u279c  {func['path']}\")\n\n    if internet_search:\n        queries = [\n            \"research how to build a supervisor agent system in python\",\n            \"research why the world is round\"\n        ]\n\n        for query in queries:\n            print(f\"\\n--- Research: {query} ---\")\n            result = supervisor.delegate(query)\n            print(f\"Source: {result['source']}\")\n            print(f\"Content:\\n{result['content']}\")\n\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "src\\__init__.py": "import os\nfrom flask import Flask, send_from_directory\nfrom flask_cors import CORS\ndef create_app():\n    app = Flask(__name__)\n\n    CORS(app)\n    os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n    # Register your API routes\n    from .routes import status, supervisorroute, knowledge\n    app.register_blueprint(status.bp)\n    app.register_blueprint(supervisorroute.bp)\n    app.register_blueprint(knowledge.bp)\n\n    # Serve React frontend from frontend/dist\n    @app.route(\"/\", defaults={\"path\": \"\"})\n    @app.route(\"/<path:path>\")\n    def serve_react(path):\n        dist_dir = os.path.join(os.path.dirname(__file__), \"..\", \"frontend\", \"dist\")\n        target = os.path.join(dist_dir, path)\n\n        if path != \"\" and os.path.exists(target):\n            return send_from_directory(dist_dir, path)\n        else:\n            return send_from_directory(dist_dir, \"index.html\")\n\n    return app\n",
  "src\\model\\base_agent.py": "# src/model/base_agent.py\n\nclass BaseAgent:\n    def __init__(self, name: str):\n        self.name = name\n\n    def can_handle(self, task: str) -> bool:\n        \"\"\"Determine if this agent can handle the given task.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n\n    def handle(self, task: str, **kwargs):\n        \"\"\"Handle the task and return a response.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
  "src\\model\\llm_client.py": "# src/model/llm_client.py\n\nimport os\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass LLMClient:\n    def __init__(self):\n        self.base_url = os.getenv(\"LLM_ENDPOINT\")\n        self.api_key = os.getenv(\"LLM_API_KEY\")\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"  # Try \"Api-Key\" if this fails\n        }\n\n    def query(self, user_prompt: str, system_prompt: str = \"You are a helpful assistant.\", **kwargs) -> str:\n        payload = {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            \"temperature\": 0.2,\n            \"max_tokens\": 2048,\n            **kwargs\n        }\n        try:\n            response = requests.post(self.base_url, json=payload, headers=self.headers)\n            response.raise_for_status()\n            data = response.json()\n            return data[\"choices\"][0][\"message\"][\"content\"]\n        except requests.RequestException as e:\n            print(f\"[LLMClient] Error: {e}\")\n            return f\"Error querying LLM: {e}\"\n",
  "src\\model\\supervisor.py": "# src/model/supervisor.py\n\nfrom src.model.base_agent import BaseAgent\nfrom src.model.agents.research_agent import ResearchAgent\nfrom src.model.agents.git_agent import GitAgent\nfrom src.model.llm_client import LLMClient\nimport os\n\nclass SupervisorAgent:\n    def __init__(self):\n        self.agents: list[BaseAgent] = []\n\n        # Setup: shared in-memory DB och LLM klient\n        self.database = {}\n        self.llm = LLMClient()\n\n        # Registrera underagenter\n        self.register_agent(ResearchAgent(self.database, self.llm))\n\n        # S\u00e4tt repo-s\u00f6kv\u00e4g (anv\u00e4nder projektets rot som default)\n        repo_path = os.path.abspath(\".\")\n        self.register_agent(GitAgent(repo_path, self.llm))\n\n    def register_agent(self, agent: BaseAgent):\n        self.agents.append(agent)\n\n    def decide_agent(self, task: str) -> BaseAgent | None:\n        prompt = (\n            f\"You are a routing assistant in a multi-agent system.\\n\"\n            f\"Here is the user task:\\n\\n\\\"{task}\\\"\\n\\n\"\n            \"Based on this, which of the following agents should handle it?\\n\"\n            \"- ResearchAgent: answers questions or gathers external information\\n\"\n            \"- GitAgent: explains code, analyzes commits, explores repositories\\n\\n\"\n            \"Respond with only the agent name: ResearchAgent or GitAgent.\"\n        )\n        decision = self.llm.query(prompt).strip()\n        return next((a for a in self.agents if a.name == decision), None)\n\n    def delegate(self, task: str, **kwargs):\n        for agent in self.agents:\n            if agent.can_handle(task):\n                print(f\"[Supervisor] Delegating to {agent.name} via keyword match\")\n                result = agent.handle(task, **kwargs)\n                return self._validate_semantic_match(task, result)\n        # Fallback: anv\u00e4nd LLM f\u00f6r att best\u00e4mma\n        selected = self.decide_agent(task)\n        if selected:\n            print(f\"[Supervisor] Delegating to {selected.name} via LLM decision\")\n            result = selected.handle(task, **kwargs)\n            return self._validate_semantic_match(task, result)\n        raise ValueError(f\"No agent found to handle task: {task}\")\n\n    def _validate_semantic_match(self, task: str, result):\n        # Om resultatet inte \u00e4r en dictionary, returnera det direkt.\n        if not isinstance(result, dict):\n            return result\n\n        source = result.get(\"source\", \"\")\n        if source.startswith(\"semantic match:\"):\n            matched_query = source.split(\"semantic match:\")[1].strip()\n            content = result.get(\"content\", \"\")[:500]  # F\u00f6rhandsvisning\n            prompt = (\n                f\"A similar research entry was found for the query '{task}':\\n\\n\"\n                f\"Matched Query: {matched_query}\\n\"\n                f\"Matched Content Preview:\\n{content}\\n\\n\"\n                \"Is this relevant to the current question? Respond only YES or NO.\"\n            )\n            verdict = self.llm.query(prompt).strip().lower()\n            if \"yes\" in verdict:\n                return result\n            else:\n                print(\"[Supervisor] Semantic match rejected by LLM \ud83d\udc4e\")\n                # Fallback: hitta ResearchAgent igen och tvinga fram en internets\u00f6kning\n                research_agent = next((a for a in self.agents if a.name == \"ResearchAgent\"), None)\n                if research_agent:\n                    print(\"[Supervisor] Falling back to internet search via ResearchAgent \ud83c\udf10\")\n                    return research_agent.handle(task, force_internet=True)\n                return {\"source\": \"supervisor\", \"content\": \"No relevant cached information and internet search is unavailable.\"}\n        return result\n",
  "src\\model\\__init__.py": "",
  "src\\model\\agents\\git_agent.py": "# src/model/agents/git_agent.py\n\nimport subprocess\nimport os\nimport re\nimport ast\nfrom src.model.base_agent import BaseAgent\nfrom src.model.llm_client import LLMClient\nfrom src.model.utils.file_indexer import index_repo_files\n\nclass GitAgent(BaseAgent):\n    def __init__(self, repo_path: str, llm_client: LLMClient):\n        super().__init__(\"GitAgent\")\n        self.repo_path = repo_path\n        self.llm = llm_client\n        self.file_index = index_repo_files(self.repo_path)\n\n    def can_handle(self, task: str) -> bool:\n        task = task.lower()\n        return any(keyword in task for keyword in [\n            \"git\", \"code review\", \"commit\", \"project overview\", \"suggest improvement\"\n        ])\n\n    def handle(self, task: str, **kwargs):\n        if \"summary\" in task:\n            return self.summarize_latest_commit()\n        elif \"project overview\" in task:\n            return self.project_overview()\n        elif \"suggest improvement\" in task:\n            return self.suggest_improvements()\n        else:\n            return \"GitAgent: Task not recognized.\"\n\n    def summarize_latest_commit(self):\n        diff = self._get_latest_commit_diff()\n        prompt = f\"Review and summarize this Git commit:\\n\\n{diff}\"\n        return self.llm.query(prompt)\n\n    def project_overview(self):\n        structure = self._get_directory_structure()\n        prompt = f\"This is the file structure of a codebase:\\n\\n{structure}\\n\\nWhat is this project likely doing?\"\n        return self.llm.query(prompt)\n\n    def suggest_improvements(self):\n        structure = self._get_directory_structure()\n        prompt = f\"Here is the project structure:\\n\\n{structure}\\n\\nSuggest possible improvements to structure or code quality.\"\n        return self.llm.query(prompt)\n    \n    def print_file_index_preview(self, limit: int = 3, chars: int = 300):\n        print(f\"\\n[GitAgent] Previewing first {limit} indexed files:\\n\")\n        count = 0\n        for path, content in self.file_index.items():\n            print(f\"--- {path} ---\")\n            print(content[:chars] + (\"...\" if len(content) > chars else \"\"))\n            print()\n            count += 1\n            if count >= limit:\n                break\n\n        if count == 0:\n            print(\"No files were indexed.\")\n\n\n    def reindex_files(self):\n        from src.model.utils.file_indexer import index_repo_files\n        self.file_index = index_repo_files(self.repo_path, force_refresh=True)\n        print(\"[GitAgent] File index refreshed and cached.\")\n\n    def explain_function(self, function_name: str) -> str:\n            for path, content in self.file_index.items():\n                try:\n                    tree = ast.parse(content)\n                    for node in ast.walk(tree):\n                        if isinstance(node, ast.FunctionDef) and node.name == function_name:\n                            # Get full source code lines\n                            lines = content.splitlines()\n                            start_line = node.lineno - 1\n                            end_line = getattr(node, 'end_lineno', start_line + 1)\n\n                            # Extract function code block\n                            function_code = \"\\n\".join(lines[start_line:end_line])\n\n                            prompt = (\n                                f\"Here's a Python function called `{function_name}` from the file `{path}`:\\n\\n\"\n                                f\"```python\\n{function_code.strip()}\\n```\\n\\n\"\n                                \"Please explain in detail what this function does.\"\n                            )\n                            return self.llm.query(prompt)\n                except SyntaxError:\n                    continue\n\n            return f\"Function `{function_name}` not found in indexed files.\"\n    \n    def list_all_functions(self) -> list[dict]:\n        functions = []\n\n        for path, content in self.file_index.items():\n            try:\n                tree = ast.parse(content)\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.FunctionDef):\n                        functions.append({\n                            \"name\": node.name,\n                            \"path\": path\n                        })\n            except SyntaxError:\n                continue\n\n        return functions\n\n    def _get_latest_commit_diff(self) -> str:\n        try:\n            result = subprocess.run(\n                [\"git\", \"show\", \"--stat\", \"--unified=1\"],\n                cwd=self.repo_path,\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            return result.stdout\n        except subprocess.CalledProcessError as e:\n            return f\"[GitAgent Error] Could not retrieve commit diff:\\n{e.stderr or str(e)}\"\n\n\n    def _get_directory_structure(self) -> str:\n        output = []\n        for root, dirs, files in os.walk(self.repo_path):\n            depth = root.replace(self.repo_path, \"\").count(os.sep)\n            indent = \"  \" * depth\n            output.append(f\"{indent}{os.path.basename(root)}/\")\n            for f in files:\n                output.append(f\"{indent}  {f}\")\n        return \"\\n\".join(output)\n",
  "src\\model\\agents\\research_agent.py": "import uuid\nfrom datetime import datetime\nfrom src.model.base_agent import BaseAgent\nfrom src.model.llm_client import LLMClient\nfrom src.model.tools.internet_search import search_duckduckgo\nfrom src.model.utils.mongo_client import find_research, save_research\nfrom src.model.utils.embedding import get_embedding_from_llm\nfrom src.model.utils.mongo_client import vector_store, collection\nfrom src.model.utils.chunking import chunk_text\n\nclass ResearchAgent(BaseAgent):\n    def __init__(self, database: dict, llm_client: LLMClient):\n        super().__init__(\"ResearchAgent\")\n        self.database = database\n        self.llm = llm_client\n\n    def can_handle(self, task: str) -> bool:\n        return \"research\" in task.lower()\n\n    def handle(self, task: str, force_internet: bool = False, **kwargs):\n        query = kwargs.get(\"query\", task)\n\n        if not force_internet:\n            result = self.search_database(query)\n            if self.is_enough_info(result):\n                print(f\"[ResearchAgent] Found exact cached result for '{query}' \u2705\")\n                return {\"source\": \"database\", \"content\": result}\n\n            # H\u00e4mta embedding f\u00f6r sj\u00e4lva fr\u00e5gan\n            embedding = get_embedding_from_llm(query)\n            similar = vector_store.search(embedding, top_k=1)\n            if similar:\n                best = similar[0]\n                print(f\"[ResearchAgent] Found similar result via vector search: '{best['query']}' \ud83e\udd1d\")\n                # F\u00f6rs\u00f6k h\u00e4mta partition_id fr\u00e5n metadatan\n                partition_id = best.get(\"metadata\", {}).get(\"partition_id\")\n                if partition_id:\n                    # H\u00e4mta alla dokument f\u00f6r denna partition, sortera och aggregera inneh\u00e5llet\n                    docs = list(collection.find({\"partition_id\": partition_id}).sort(\"chunk_index\", 1))\n                    aggregated_content = \"\\n\\n\".join(doc[\"chunk\"] for doc in docs)\n                    return {\n                        \"source\": f\"semantic match: {best['query']}\",\n                        \"content\": aggregated_content\n                    }\n                else:\n                    # Fallback om partition_id inte finns\n                    return {\n                        \"source\": f\"semantic match: {best['query']}\",\n                        \"content\": best.get(\"chunk\", \"\")\n                    }\n\n        print(f\"[ResearchAgent] No match found or forced internet search, searching internet \ud83c\udf10\")\n        search_results = search_duckduckgo(query, max_results=5)\n        combined_snippets = \"\\n\\n\".join(search_results)\n        prompt = (\n            f\"You are a research assistant. Based on the following web search results, \"\n            f\"write a detailed summary about: '{query}'\\n\\n{combined_snippets}\"\n        )\n        summary = self.llm.query(prompt)\n        self.save_to_database(query, summary)\n        return {\"source\": \"internet\", \"content\": summary}\n\n    def search_database(self, query: str) -> str:\n        return find_research(query, max_age_days=7)\n\n    def is_enough_info(self, result: str) -> bool:\n        return bool(result and len(result.strip()) > 50)\n\n    def save_to_database(self, query: str, result: str):\n        partition_id = str(uuid.uuid4())\n        chunks = chunk_text(result)\n\n        for i, chunk in enumerate(chunks):\n            embedding = get_embedding_from_llm(chunk)\n\n            doc = {\n                \"query\": query,\n                \"chunk\": chunk,\n                \"chunk_index\": i,\n                \"embedding\": embedding,\n                \"partition_id\": partition_id,\n                \"updated_at\": datetime.utcnow(),\n            }\n            collection.insert_one(doc)\n            # L\u00e4gg till partition_id i metadatan s\u00e5 att vi kan identifiera hela posten vid senare s\u00f6kningar\n            vector_store.add_entry(\n                query=f\"{query} (chunk {i})\",\n                embedding=embedding,\n                metadata={\"partition_id\": partition_id}\n            )\n",
  "src\\model\\agents\\__init__.py": "",
  "src\\model\\tools\\internet_search.py": "# src/model/tools/internet_search.py\n\nimport requests\nfrom duckduckgo_search import DDGS\nfrom bs4 import BeautifulSoup\nimport urllib.parse\nimport time\nprint(\"init\")\nHEADERS = {\n    \"User-Agent\": (\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n        \"Chrome/114.0.0.0 Safari/537.36\"\n    )\n}\n\ndef search_duckduckgo(query: str, max_results: int = 5) -> list[str]:\n    try:\n        with DDGS() as ddgs:\n            results = ddgs.text(query, max_results=max_results)\n            return [\n                f\"{r['title']} - {r['body']}\\n{r['href']}\"\n                for r in results\n            ]\n    except Exception as e:\n        print(f\"[DuckDuckGoSearch] Error: {e}\")\n        return [f\"[Search Error] {e}\"]",
  "src\\model\\utils\\chunking.py": "from src.model.utils.embedding import tokenizer  # \u00e5teranv\u00e4nd tokenizer fr\u00e5n embedding.py\n\ndef chunk_text(text: str, max_tokens: int = 100):\n    sentences = text.split(\". \")\n    chunks = []\n    current_chunk = []\n\n    for sentence in sentences:\n        current_chunk.append(sentence.strip())\n        tokenized = tokenizer(\" \".join(current_chunk), return_tensors=\"pt\", truncation=False)\n        if tokenized.input_ids.shape[1] > max_tokens:\n            current_chunk.pop()\n            chunks.append(\". \".join(current_chunk) + \".\")\n            current_chunk = [sentence.strip()]\n\n    if current_chunk:\n        chunks.append(\". \".join(current_chunk) + \".\")\n\n    return chunks",
  "src\\model\\utils\\embedding.py": "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# Load once\ntokenizer = AutoTokenizer.from_pretrained(\"KBLab/bert-base-swedish-cased\")\nmodel = AutoModel.from_pretrained(\"KBLab/bert-base-swedish-cased\")\n\ndef get_embedding_from_llm(text: str) -> list[float]:\n    inputs = tokenizer(\n    text,\n    return_tensors=\"pt\",\n    truncation=True,\n    padding=True,\n    max_length=512\n)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Mean pooling\n    token_embeddings = outputs.last_hidden_state.squeeze(0)\n    attention_mask = inputs[\"attention_mask\"].squeeze(0)\n    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n    masked_embeddings = token_embeddings * mask\n    summed = torch.sum(masked_embeddings, dim=0)\n    count = torch.clamp(mask.sum(dim=0), min=1e-9)\n    mean_pooled = summed / count\n\n    return mean_pooled.tolist()\n",
  "src\\model\\utils\\file_indexer.py": "# src/model/utils/file_indexer.py\nimport os\nimport json\n\nEXCLUDED_DIRS = {\".git\", \".venv\", \"__pycache__\", \"node_modules\", \"data\"}\nEXCLUDED_FILES = {\".env\"}\nCACHE_FILE = \"data/file_index.json\"\n\ndef load_gitignore_patterns(repo_path: str) -> list:\n    gitignore_path = os.path.join(repo_path, \".gitignore\")\n    patterns = []\n    if os.path.exists(gitignore_path):\n        with open(gitignore_path, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if line and not line.startswith(\"#\"):\n                    patterns.append(line)\n    return patterns\n\n\ndef is_ignored(path: str, patterns: list) -> bool:\n    parts = path.split(os.sep)\n    \n    if any(part in EXCLUDED_DIRS for part in parts):\n        return True\n    if os.path.basename(path) in EXCLUDED_FILES:\n        return True\n    for pattern in patterns:\n        if pattern in path:\n            return True\n    return False\n\ndef index_repo_files(repo_path: str, force_refresh=False) -> dict:\n    os.makedirs(\"data\", exist_ok=True)\n\n    if os.path.exists(CACHE_FILE) and not force_refresh:\n        with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    file_index = {}\n    gitignore = load_gitignore_patterns(repo_path)\n\n    for root, _, files in os.walk(repo_path):\n        for filename in files:\n            rel_path = os.path.relpath(os.path.join(root, filename), repo_path)\n\n            if is_ignored(rel_path, gitignore):\n                continue\n\n            try:\n                with open(os.path.join(root, filename), \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n                    file_index[rel_path] = content\n            except (UnicodeDecodeError, FileNotFoundError):\n                continue\n\n    # Save to cache\n    with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n        json.dump(file_index, f, indent=2)\n\n    return file_index",
  "src\\model\\utils\\mongo_client.py": "from datetime import datetime, timedelta\nfrom pymongo import MongoClient\nimport os\nfrom src.model.vector_store.vector_store import VectorStore\nfrom src.model.utils.embedding import get_embedding_from_llm  # se till att denna \u00e4r korrekt importerad\n\nMONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://admin:supersecret@localhost:27017/\")\nclient = MongoClient(MONGO_URI)\n\ndb = client[\"multiagent\"]\ncollection = db[\"research_cache\"]\n\n# \u2705 Initialize vector store from Mongo data\nvector_store = VectorStore(collection)\n\ndef save_research(query: str, content: str, embedding=None):\n    update_data = {\n        \"content\": content,\n        \"updated_at\": datetime.utcnow()\n    }\n    if embedding:\n        update_data[\"embedding\"] = embedding\n\n    collection.update_one(\n        {\"query\": query},\n        {\"$set\": update_data},\n        upsert=True\n    )\n\n    if embedding:\n        # Vid sparande av nya chunkade poster b\u00f6r du se till att vector_store.add_entry anropas med metadata\n        vector_store.add_entry(query, embedding, metadata={})\n        \ndef find_research(query: str, max_age_days: int = 7) -> str:\n    \"\"\"\n    Uppdaterad implementation som anv\u00e4nder semantisk s\u00f6kning via FAISS-indexet.\n    Ber\u00e4knar embedding f\u00f6r query, s\u00f6ker i vector_store, och om en match hittas\n    s\u00e5 h\u00e4mtas hela posten (alla chunkar) via partition_id och aggregeras.\n    \"\"\"\n    # Ber\u00e4kna embedding f\u00f6r fr\u00e5gan\n    embedding = get_embedding_from_llm(query)\n    # S\u00f6k efter matchande poster via FAISS\n    results = vector_store.search(embedding, top_k=1, threshold=0.85)\n    if results:\n        best = results[0]\n        metadata = best.get(\"metadata\", {})\n        partition_id = metadata.get(\"partition_id\")\n        if partition_id:\n            # H\u00e4mta alla dokument med samma partition_id och sortera p\u00e5 chunk_index\n            docs = list(collection.find({\"partition_id\": partition_id}).sort(\"chunk_index\", 1))\n            aggregated_content = \"\\n\\n\".join(d.get(\"chunk\", \"\") for d in docs)\n            # Eventuellt: \u00e4ven kolla om posten inte \u00e4r f\u00f6r gammal\n            if docs:\n                updated_at = docs[-1].get(\"updated_at\")\n                if updated_at and datetime.utcnow() - updated_at > timedelta(days=max_age_days):\n                    return \"\"\n            return aggregated_content\n        else:\n            # Om inget partition_id finns, returnera det enskilda dokumentets inneh\u00e5ll\n            return best.get(\"content\", best.get(\"chunk\", \"\"))\n    return \"\"\n",
  "src\\model\\vector_store\\vector_store.py": "import faiss\nimport numpy as np\nfrom pymongo.collection import Collection\n\ndef normalize_embedding(embedding: list[float]) -> np.ndarray:\n    \"\"\"Converts list to float32 numpy array and normalizes it.\"\"\"\n    arr = np.array(embedding).astype(\"float32\")\n    norm = np.linalg.norm(arr)\n    if norm > 0:\n        return arr / norm\n    return arr\n\nclass VectorStore:\n    def __init__(self, mongo_collection: Collection):\n        self.mongo_collection = mongo_collection\n        self.index = None\n        self.mapping = {}  # maps FAISS index positions to a dict containing query and metadata\n        self._initialize_index()\n\n    def _initialize_index(self):\n        print(\"[VectorStore] Loading embeddings from MongoDB...\")\n        embeddings = []\n        self.mapping = {}  # Reset mapping\n\n        for i, doc in enumerate(self.mongo_collection.find({\"embedding\": {\"$exists\": True}})):\n            vec = doc[\"embedding\"]\n            if not vec:\n                continue\n            norm_vec = normalize_embedding(vec)\n            embeddings.append(norm_vec)\n            # H\u00e4r lagras endast query (metadata saknas i befintliga dokument)\n            self.mapping[i] = {\"query\": doc[\"query\"], \"metadata\": {}}\n\n        if embeddings:\n            dim = len(embeddings[0])\n            self.index = faiss.IndexFlatIP(dim)\n            self.index.add(np.array(embeddings))\n            print(f\"[VectorStore] Loaded {len(embeddings)} vectors into FAISS.\")\n        else:\n            self.index = None\n            print(\"[VectorStore] No embeddings found in DB.\")\n\n    def search(self, query_vector: list[float], top_k=3, threshold=0.8) -> list[dict]:\n        if not self.index:\n            return []\n\n        norm_query = normalize_embedding(query_vector)\n        query_np = np.array([norm_query])\n        distances, indices = self.index.search(query_np, top_k)\n\n        results = []\n        for idx, sim in zip(indices[0], distances[0]):\n            if idx in self.mapping and sim >= threshold:\n                mapping_entry = self.mapping[idx]\n                query_val = mapping_entry.get(\"query\", \"\")\n                metadata = mapping_entry.get(\"metadata\", {})\n                doc = self.mongo_collection.find_one({\"query\": query_val})\n                if doc:\n                    results.append({\n                        \"query\": doc[\"query\"],\n                        \"content\": doc.get(\"content\", doc.get(\"chunk\", \"\")),\n                        \"metadata\": metadata,\n                        \"distance\": float(sim)\n                    })\n        return results\n\n    def add_entry(self, query: str, embedding: list[float], metadata: dict = None):\n        norm_embedding = normalize_embedding(embedding)\n        if not self.index:\n            self.index = faiss.IndexFlatIP(len(norm_embedding))\n        idx = self.index.ntotal\n        self.index.add(np.array([norm_embedding]))\n        self.mapping[idx] = {\"query\": query, \"metadata\": metadata or {}}\n\n    def reindex(self):\n        self._initialize_index()\n        print(\"[VectorStore] FAISS index reinitialized.\")\n",
  "src\\routes\\knowledge.py": "from flask import Blueprint, jsonify, request\nfrom src.model.utils.mongo_client import collection, vector_store    \nfrom src.model.utils.chunking import chunk_text\nfrom src.model.utils.embedding import get_embedding_from_llm\nfrom datetime import datetime\n\nbp = Blueprint(\"knowledge\", __name__, url_prefix=\"/api\")\n\n@bp.route(\"/knowledge\", methods=[\"GET\"])\ndef get_knowledge():\n    docs = collection.find(\n        {\"chunk\": {\"$exists\": True}}, \n        {\"_id\": 0, \"query\": 1, \"chunk\": 1, \"chunk_index\": 1, \"partition_id\": 1, \"updated_at\": 1}\n    )\n\n    grouped = {}\n    for doc in docs:\n        pid = doc[\"partition_id\"]\n        if pid not in grouped:\n            grouped[pid] = {\n                \"query\": doc[\"query\"],\n                \"chunks\": [],\n                \"updated_at\": doc[\"updated_at\"]\n            }\n        grouped[pid][\"chunks\"].append((doc[\"chunk_index\"], doc[\"chunk\"]))\n        # Uppdatera till senaste updated_at om nyare\n        if doc[\"updated_at\"] > grouped[pid][\"updated_at\"]:\n            grouped[pid][\"updated_at\"] = doc[\"updated_at\"]\n\n    results = []\n    for pid, data in grouped.items():\n        # Sortera chunks baserat p\u00e5 chunk_index\n        sorted_chunks = sorted(data[\"chunks\"], key=lambda x: x[0])\n        # Skapa en array med objekt med chunk_index och content\n        chunk_list = [{\"chunk_index\": idx, \"content\": cnt} for idx, cnt in sorted_chunks]\n        results.append({\n            \"partition_id\": pid,\n            \"query\": data[\"query\"],\n            \"updated_at\": data[\"updated_at\"],\n            \"chunks\": chunk_list\n        })\n\n    return jsonify(results)\n\n@bp.route(\"/knowledge/<query>\", methods=[\"PATCH\"])\ndef update_knowledge(query):\n    data = request.get_json()\n    new_content = data.get(\"content\")\n\n    if not new_content:\n        return jsonify({\"error\": \"Missing content\"}), 400\n\n    # H\u00e4mta partition_id f\u00f6r denna query\n    first_doc = collection.find_one({\"query\": query})\n    if not first_doc:\n        return jsonify({\"error\": \"Entry not found\"}), 404\n\n    partition_id = first_doc[\"partition_id\"]\n\n    # Ta bort gamla chunkar\n    collection.delete_many({\"partition_id\": partition_id})\n\n    # Chunka nytt content\n    chunks = chunk_text(new_content)\n    for i, chunk in enumerate(chunks):\n        embedding = get_embedding_from_llm(chunk)\n        doc = {\n            \"query\": query,\n            \"chunk\": chunk,\n            \"chunk_index\": i,\n            \"embedding\": embedding,\n            \"partition_id\": partition_id,\n            \"updated_at\": datetime.utcnow()\n        }\n        collection.insert_one(doc)\n        vector_store.add_entry(f\"{query} (chunk {i})\", embedding)\n        vector_store.reindex()\n\n    return jsonify({\"message\": \"Entry updated\"})\n\n@bp.route(\"/knowledge/<query>\", methods=[\"DELETE\"])\ndef delete_knowledge(query):\n    doc = collection.find_one({\"query\": query})\n    if not doc:\n        return jsonify({\"error\": \"Entry not found\"}), 404\n\n    partition_id = doc[\"partition_id\"]\n    result = collection.delete_many({\"partition_id\": partition_id})\n    vector_store.reindex()\n    return jsonify({\"message\": f\"Deleted {result.deleted_count} chunks.\"})\n\n@bp.route(\"/knowledge\", methods=[\"DELETE\"])\ndef delete_all_knowledge():\n    result = collection.delete_many({})\n    vector_store.reindex()\n    return jsonify({\"message\": f\"Deleted {result.deleted_count} entries\"})\n",
  "src\\routes\\status.py": "# src/routes/status.py\nfrom flask import Blueprint, jsonify\n\nbp = Blueprint('status', __name__, url_prefix='/status')\n\n@bp.route('/', methods=['GET'])\ndef get_status():\n    return jsonify({\"status\": \"ok\"})\n",
  "src\\routes\\supervisorroute.py": "# src/routes/supervisorroute.py\n\nfrom flask import Blueprint, request, jsonify\nfrom src.model.supervisor import SupervisorAgent\n\nbp = Blueprint(\"supervisor\", __name__, url_prefix=\"/api\")\nsupervisor_agent = SupervisorAgent()  # Se till att agenten initieras korrekt\n\n@bp.route(\"/ask-supervisor\", methods=[\"POST\"])\ndef ask_supervisor():\n    data = request.get_json() or {}\n    # Byt h\u00e4r: vi v\u00e4ntar p\u00e5 \"task\" ist\u00e4llet f\u00f6r \"content\"\n    task = data.get(\"task\")\n    if not task:\n        return jsonify({\"error\": \"Missing task\"}), 400\n\n    try:\n        result = supervisor_agent.delegate(task)\n        return jsonify(result)\n    except Exception as e:\n        # F\u00f6r att f\u00e5nga eventuella ov\u00e4ntade fel\n        return jsonify({\"error\": str(e)}), 500\n",
  "src\\routes\\__init__.py": "",
  "tests_backend\\test_llm.py": "# test_llm.py\n\nfrom src.model.llm_client import LLMClient\n\ndef main():\n    llm = LLMClient()\n    prompt = \"Tell me a fun fact about space.\"\n    response = llm.query(prompt)\n    print(f\"LLM Response:\\n{response}\")\n\nif __name__ == \"__main__\":\n    main()\n"
}